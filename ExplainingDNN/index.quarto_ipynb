{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"LeNet-Driven CNN Explainability through ExcitationBP Heat-Maps and Sparse Concepts using XNN + SRAE\"\n",
        "author: \"Brian Cervantes Alvarez\"\n",
        "date: \"2025-06-06\"\n",
        "date-format: full\n",
        "format:\n",
        "  revealjs:\n",
        "    theme: simple\n",
        "    slide-number: true\n",
        "    scrollable: true\n",
        "html-math-method: mathjax\n",
        "lightbox: true\n",
        "crossref:\n",
        "  fig-title: Fig\n",
        "  tbl-title: Tbl\n",
        "  title-delim: \"—\"\n",
        "eval: false\n",
        "---\n",
        "\n",
        "\n",
        "## Agenda {data-background-color=\"#D73F09\" .smaller} \n",
        "\n",
        "1. *Motivation and risk*—Why care about explanations?\n",
        "2. **Convolutional neural network (CNN) architecture** overview  \n",
        "   * Layers and operations  \n",
        "   * Training considerations\n",
        "3. **XNN + SRAE**—concept extraction\n",
        "4. **Causal localisation** (I-GOS++)\n",
        "5. **Ethical considerations & Future research**\n",
        "6. **Questions and discussion**\n",
        "\n",
        "\n",
        "## 1. Motivation and Risk — Why Care About Explanations? {.smaller}\n",
        "\n",
        "* **Safety-critical decisions**  \n",
        "  * Autonomous vehicles: distinguish a **red traffic light** from a red billboard  \n",
        "  * Medical diagnostics: pathologists must identify the specific cells that trigger a cancer flag  \n",
        "* **Legal and ethical compliance**  \n",
        "  * EU AI Act, FDA SaMD guidelines require interpretability  \n",
        "* **Model debugging and improvement**  \n",
        "  * Detect shortcut learning (e.g., background colour in bird identification)  \n",
        "  * Reveal dataset bias and temporal drift  \n",
        "\n",
        "Opaque decision pathways can introduce risk, undermine confidence, and complicate audits.\n",
        "\n",
        "\n",
        "# 2. CNN Architecture Refresher {data-background-color=\"#D73F09\"}\n",
        "\n",
        "## Building Blocks of a CNN {.smaller}\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"40%\"}\n",
        "1. **Convolution** (+ ReLU)  \n",
        "2. **Stride / Padding**  \n",
        "3. **Pooling** (max & average)  \n",
        "4. **Normalisation** (batch / layer)  \n",
        "5. **Residual connections**  \n",
        "6. **Flatten → dense head**  \n",
        "7. **Regularisation** (dropout)\n",
        ":::\n",
        "::: {.column width=\"60%\"}\n",
        "![Figure 1. High-level CNN pipeline](images/article-hero-cnn.webp)\n",
        ":::\n",
        "::::\n",
        "\n",
        "## Convolution Kernels {.smaller}\n",
        "\n",
        "![Figure 2. Kernel illustration](images/kernel_filter.webp)\n",
        "\n",
        "A **kernel** (filter) traverses the image, computing local dot-products to detect edges, textures, or colour blobs.  \n",
        "Output spatial size: $\\bigl[i - k\\bigr] + 1$\n",
        "\n",
        "## Stride & Padding {.smaller}\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "![Figure 3. Stride illustration](images/stride.webp)\n",
        "\n",
        "Stride $s$ controls down-sampling:  \n",
        "$\\bigl[i - k\\bigr] / s + 1$\n",
        ":::\n",
        "::: {.column width=\"50%\"}\n",
        "![Figure 4. Padding illustration](images/padding.webp)\n",
        "\n",
        "Zero-padding $p$ preserves border context:  \n",
        "$\\bigl[i - k + 2p\\bigr] / s + 1$\n",
        ":::\n",
        "::::\n",
        "\n",
        "## Pooling Layers {.smaller}\n",
        "\n",
        "![Figure 5. Max vs average pooling](images/pooling.webp)\n",
        "\n",
        "* **Max pooling:** retains dominant activation, emphasising edge detectors  \n",
        "* **Average pooling:** provides a smoother summary, capturing global context  \n",
        "\n",
        "Both improve translation invariance and reduce computation.\n",
        "\n",
        "## Normalization in CNNs {.smaller}\n",
        "\n",
        "After each convolution, the distribution of activations can shift (“internal covariate shift”), slowing learning. Normalization keeps activations on a stable scale so the network converges faster. For each channel and each mini-batch:\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"33.3%\"}\n",
        "Compute the batch mean  \n",
        "$$\n",
        "\\mu = \\frac{1}{m}\\sum_{i=1}^m x_i\n",
        "$$\n",
        ":::\n",
        "::: {.column width=\"33.3%\"}\n",
        "Compute the batch variance  \n",
        "$$\n",
        "\\sigma^2 = \\frac{1}{m}\\sum_{i=1}^m (x_i - \\mu)^2\n",
        "$$\n",
        ":::\n",
        "::: {.column width=\"33.3%\"}\n",
        "Standardize and re‐scale  \n",
        "$$\n",
        "\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}, \\quad\n",
        "y_i = \\gamma\\,\\hat{x}_i + \\beta\n",
        "$$\n",
        ":::\n",
        "::::\n",
        "\n",
        "$\\epsilon$ avoids division by zero & $\\gamma,\\beta$ are learnable scale and shift.\n",
        "\n",
        "* **Faster convergence:** gradients stay well‐scaled  \n",
        "* **Milder sensitivity** to initialization and learning rate  \n",
        "* **Regularization effect:** slight noise from batch estimates helps generalization\n",
        "\n",
        "\n",
        "## Residual Connections (ResNets) {.smaller}\n",
        "\n",
        "Deep networks suffer from vanishing gradients; stacking more layers can *worsen* training accuracy. A shortcut (identity) path lets the block learn a *residual* function:\n",
        "\n",
        "$$\n",
        "\\mathbf{y} = \\mathcal{F}(\\mathbf{x}) + \\mathbf{x}\n",
        "$$\n",
        "\n",
        "* $\\mathbf{x}$: input to the block  \n",
        "* $\\mathcal{F}(\\mathbf{x})$: the “residual” (e.g., Conv → ReLU → Conv)  \n",
        "\n",
        "* Enables gradients to flow directly through the identity branch  \n",
        "* Lets layers fit small corrections ($\\mathcal{F}$) rather than full mappings  \n",
        "* Empirically allows networks of hundreds of layers to train  \n",
        "\n",
        "\n",
        "## Flatten & Fully Connected Layers {.smaller}\n",
        "\n",
        "![Figure 6. Flatten illustration](images/flatten.webp)\n",
        "\n",
        "The three-dimensional tensor ($H\\times W\\times K$) is flattened and passed to dense layers, which integrate global context and output class probabilities via softmax.\n",
        "\n",
        "\n",
        "## Regularization: Dropout {.smaller}\n",
        "\n",
        "![Figure 7. Dropout mask](images/dropout.webp)\n",
        "\n",
        "Randomly deactivates units during training to discourage co-adaptation and mitigate overfitting.\n",
        "\n",
        "\n",
        "# 3. From CNN Architecture to XNN Explanation {data-background-color=\"#D73F09\"}\n",
        "\n",
        "## What is XNN? {.smaller}\n",
        "\n",
        "- **XNN** = e**X**planation **N**eural **N**etwork  \n",
        "- A compact network **attached** to a frozen, pre-trained CNN  \n",
        "- **Encoder** $E_\\theta$: compresses the CNN’s feature vector  \n",
        "  $\\displaystyle \\mathbf{z}\\in\\mathbb{R}^D \\;\\longmapsto\\; \\mathbf{e}=E_\\theta(\\mathbf{z})\\in\\mathbb{R}^L$  \n",
        "- **Projection** $v$: lifts $\\mathbf{e}$ back to the original output space  \n",
        "  $\\displaystyle \\hat{\\mathbf{y}}=v^\\top\\mathbf{e}$  \n",
        "- Trained with three losses to enforce:  \n",
        "  1. **Faithfulness** (match original predictions)  \n",
        "  2. **Sparsity** (keep concepts concise)  \n",
        "  3. **Orthogonality** (make concepts distinct)  \n",
        "- **Gives us:** sparse, stable, and interpretable “concept activations” you can visualize and audit  \n",
        "\n",
        "\n",
        "## What is XNN? {.smaller}\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "![Figure 8.](images/mainPaper1.png){width=80%}\n",
        "\n",
        "- The explanation network takes $Z$, squeezes it down to just a few important “concept” numbers (the low-dimensional $E$), and uses those to reproduce the same prediction—so we can see which few concepts the CNN really cares about.\n",
        "\n",
        "\n",
        ":::\n",
        "::: {.column width=\"50%\"}\n",
        "![Figure 9.](images/mainPaper2.png){width=80%}\n",
        "\n",
        "- First, the explanation network uses a small “encoder” to turn the long feature vector $Z$ into a short concept vector $E$. \n",
        "- Then a “decoder” tries to rebuild only the key parts of $Z$ (so we stay focused on a few features), a penalty makes sure each concept is different from the others, and finally a simple linear layer $v$ uses those concepts to exactly match the CNN’s original output.\n",
        ":::\n",
        "::::\n",
        "\n",
        "\n",
        "## XNN + SRAE Loss Components {.smaller}\n",
        "\n",
        "For a batch of $M$ inputs, we train the explanation network by minimizing the composite loss function:\n",
        "\n",
        "$$\n",
        "\\boxed{\n",
        "\\mathcal{L} \\;=\\; L_{\\mathrm{faith}} \\;+\\; L_{\\mathrm{SR}} \\;+\\; L_{\\mathrm{PT}}\n",
        "}\n",
        "$$\n",
        "\n",
        "balancing  \n",
        "\n",
        "1. **Faithfulness** $\\;L_{\\mathrm{faith}}$  \n",
        "2. **Sparse-Reconstruction** $\\;L_{\\mathrm{SR}}$  \n",
        "3. **Orthogonality (Pull-Away Term)** $\\;L_{\\mathrm{PT}}$  \n",
        "\n",
        "## Understanding the XNN + SRAE Total Loss\n",
        "\n",
        "$$\n",
        "\\mathcal{L} \\;=\\; L_{\\mathrm{faith}} \\;+\\; L_{\\mathrm{SR}} \\;+\\; L_{\\mathrm{PT}}\n",
        "$$\n",
        "\n",
        "The total loss combines **three goals**: it makes the explanation network’s output match the original CNN’s predictions (faithfulness), forces it to use only a few key internal features (sparsity), and ensures each learned “concept” is different from the others (orthogonality). By minimizing this combined score, we get a small set of simple, distinct concepts that still predict as accurately as the original model.\n",
        "\n",
        "\n",
        "## (1) Faithfulness Loss $L_{\\mathrm{faith}}$ {.smaller}\n",
        "\n",
        "$$\n",
        "L_{\\mathrm{faith}}\n",
        "\\;=\\;\n",
        "\\underbrace{\\frac{1}{M}\\sum_{i=1}^{M}\\sum_{c=1}^{C}\n",
        "\\Bigl(\\bar y_{i,c} \\;-\\;\\hat y_{i,c}\\Bigr)^{2}}_{\\substack{\\text{average of how far apart}\\\\\\text{the two sets of scores are}}}\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "* $\\bar y_{i,c}$ is the **backbone’s** output score (the CNN’s own number) for class $c$ on image $i$.\n",
        "* $\\hat y_{i,c} = v^\\top E_\\theta\\bigl(Z^{(i)}\\bigr)$ is the explanation network’s score for class $c$ (the small network’s prediction).\n",
        "\n",
        "This loss measures, on average, how different the explanation network’s scores are from the CNN’s scores. By making this number small, we force the small network to copy the CNN’s predictions closely.\n",
        "\n",
        "\n",
        "\n",
        "## (1) Faithfulness Loss $\\;L_{\\mathrm{faith}}$\n",
        "\n",
        "By passing $Z$ through the five-dimensional encoder and then using $v$ to predict $\\hat{y}$, we ensure the five concepts can recreate the CNN’s original output. In practice, $\\hat{y}$ should match the softmax scores from LeNet-5’s final layer almost exactly.\n",
        "\n",
        "> **Faithfulness** should converge toward 0 (perfect matching of the CNN’s logits).\n",
        "\n",
        "\n",
        "## (2) Sparse-Reconstruction Loss $\\;L_{\\mathrm{SR}}$ {.smaller}\n",
        "\n",
        "\n",
        "$$\n",
        "L_{\\mathrm{SR}}\n",
        "\\;=\\;\n",
        "\\underbrace{\\frac{\\beta}{D_z}}_{\\substack{\\text{trade-off}\\\\\\text{weight}}}\n",
        "\\underbrace{\\sum_{k=1}^{D_z}\n",
        "\\log\\!\\Biggl(\n",
        "1 \\;+\\;\n",
        "q \\;\\underbrace{\\frac{1}{M}\\sum_{i=1}^{M}\n",
        "\\bigl(\\,\\tilde Z^{(i)}_{k} - Z^{(i)}_{k}\\bigr)^{2}}_{\\substack{\\text{average squared}\\\\\\text{reconstruction error}}}\n",
        "\\Biggr)}_{\\substack{\\text{encourages using only}\\\\\\text{a few important features}}}\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "* $Z^{(i)}_{k}$ is the $k$th coordinate of the backbone activation for image $i$.\n",
        "* $\\tilde Z^{(i)}_{k}$ is that same coordinate rebuilt by the decoder.\n",
        "* $\\beta$ controls how strongly we force sparsity (fewer features).\n",
        "* $q$ makes the penalty jump quickly if reconstruction errors get big.\n",
        "* $D_z$ is the total number of coordinates in the backbone’s activation.\n",
        "\n",
        "By taking $\\log\\bigl(1 + q \\times \\text{MSE}\\bigr)$, we give a small penalty for small errors and a larger penalty for big errors. This makes the explanation network focus on only a **few** key coordinates of $Z$, so most coordinates stay zero.\n",
        "\n",
        "\n",
        "\n",
        "## (2) Sparse-Reconstruction Loss $\\;L_{\\mathrm{SR}}$ \n",
        "\n",
        "The decoder $\\tilde{\\theta}$ is trained to reconstruct only a small subset of the 128 features (with a log‐penalty), forcing each of the five concepts to focus on a few truly important patterns (e.g., “vertical bar” or “curved loop”).\n",
        "\n",
        "> **Sparsity** should converge toward 0 (only a few features used, making explanations very concise).\n",
        "\n",
        "\n",
        "## (3) Pull-Away (Orthogonality) Loss $L_{\\mathrm{PT}}$ {.smaller}\n",
        "\n",
        "$$\n",
        "L_{\\mathrm{PT}}\n",
        "\\;=\\;\n",
        "\\underbrace{\\frac{\\eta}{L(L-1)}}_{\\substack{\\text{strength of}\\\\\\text{orthogonality}}}\n",
        "\\underbrace{\\sum_{\\substack{l,l'=1\\\\l\\neq l'}}^{L}\n",
        "\\Bigl(\\tfrac{h_{l}^{\\top}h_{l'}}{\\|h_{l}\\|\\;\\|h_{l'}\\|}\\Bigr)^{2}}_{\\substack{\\text{encourages each concept}\\\\\\text{to be distinct (non-overlapping)}}}\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "* $h_{\\ell}\\in\\mathbb{R}^M$ is the vector of activations for concept $\\ell$ across the batch.\n",
        "* $L$ is the total number of concepts (explanation dimensionality).\n",
        "* $\\eta$ controls how strongly we discourage concepts that fire together.\n",
        "\n",
        "By penalizing squared cosine similarities between every pair of concept activations, we force each concept to occupy its own “direction” in activation space, yielding a set of distinct, non-redundant explanations.\n",
        "\n",
        "\n",
        "\n",
        "## (3) Pull-Away (Orthogonality) Loss $\\;L_{\\mathrm{PT}}$ \n",
        "\n",
        "A pull-away term makes sure each of the five concept activations is different from the others, so you end up with five distinct “reasons” that the CNN used to identify a digit.\n",
        "\n",
        "> **Orthogonality** should converge toward 0 (each concept is completely distinct from the others).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## How XNN + SRAE Works {.smaller}\n",
        "\n",
        "1. **Freeze the CNN**  \n",
        "   - We keep all CNN weights fixed (no more training).  \n",
        "   - The CNN now acts as a “feature extractor” that outputs a vector $\\mathbf{z}\\in\\mathbb{R}^D$.\n",
        "\n",
        "2. **Compress to Explanation Space**  \n",
        "   - Pass $\\mathbf{z}$ through a small encoder $E_\\theta$ to get $\\mathbf{e}\\in\\mathbb{R}^L$.  \n",
        "   - Here, $L$ is small (e.g., 5), so $\\mathbf{e}$ is easy to inspect.\n",
        "\n",
        "3. **Reconstruct Key Features**  \n",
        "   - From $\\mathbf{e}$, a tiny decoder $\\tilde\\theta$ tries to rebuild **only** the important parts of $\\mathbf{z}$.  \n",
        "   - By penalizing reconstruction error with a logarithmic penalty, we force $\\mathbf{e}$ to attend to just a **few coordinates** of $\\mathbf{z}$.\n",
        "\n",
        "4. **Match the CNN’s Output**  \n",
        "   - Use a simple linear layer $v$ on $\\mathbf{e}$ to recreate the CNN’s logits.  \n",
        "   - Training adjusts $E_\\theta$, $\\tilde\\theta$, and $v$ so that the final prediction $\\hat{\\mathbf{y}}_{\\text{XNN}}$ is as close as possible to the original $\\bar{\\mathbf{y}}$.\n",
        "\n",
        "5. **Enforce Orthogonality**  \n",
        "   - While learning, we add a penalty that makes each of the $L$ concept dimensions in $\\mathbf{e}$ point in a different direction (no overlap).  \n",
        "   - This ensures each concept is unique—e.g., one concept might capture “vertical stroke,” another “curved loop,” etc.\n",
        "\n",
        "6. **Result: Explainable Concepts**  \n",
        "   - After training, each dimension of $\\mathbf{e}$ is a **human-interpretable “concept.”**  \n",
        "   - We can visualize, for example, where in the image the “vertical-stroke” concept fires, or how strongly the “loop” concept activates.  \n",
        "   - Because $\\mathbf{e}$ is low-dimensional and each dimension is sparse, it’s far easier to understand than the original $\\mathbf{z}$.\n",
        "\n",
        "\n",
        "# From Theory to Practice! {data-background-color=\"#D73F09\"}\n",
        "\n",
        "## Implementing XNN + SRAE for MNIST Dataset {.smaller}\n",
        "\n",
        "> **LeNet-5 backbone**: Its 128-dimensional feature layer is compact enough for exhaustive hyper-parameter sweeps yet still achieves ≈99 % accuracy, allowing a fully connected surrogate to produce sharper, more faithful heat-maps than a convolution-only XNN.\n",
        "\n",
        "**Testing concept stability**: We attach a five-dimensional SRAE head to LeNet-5, freeze the CNN, and train only the explanation network with our three-term loss, verifying that high-level concepts remain consistent under small input changes.\n",
        "\n",
        "\n",
        "# Results: MNIST Case Study {data-background-color=\"#D73F09\"}\n",
        "\n",
        "## CNN Performance\n",
        "\n",
        "The frozen backbone (LeNet-5) reaches a test accuracy of **99.01 %**, with validation loss plateauing early—indicating stable generalization.\n",
        "\n",
        "## Hyper-Parameter Sweep {.smaller}\n",
        "\n",
        "#### Table 1 — Hyper-parameter Sweep\n",
        "\n",
        "| Run | β | q | η | $F_{\\text{MSE}}$ | $F_{\\text{MAE}}$ | Accuracy | $Sparse_{\\text{Loss}}$ | $Ortho_{\\text{Loss}}$ |\n",
        "|:---:|:--:|:--:|:--:|:----------------:|:----------------:|:---------:|:--------------------:|:--------------------:|\n",
        "| 1   | 1.0 | 1.0 | 0.1 | 9.2875   | 2.4538   | 0.9841   | 0.7721   | 0.0028   |\n",
        "| 2   | 1.0 | 1.0 | 0.5 | 9.2945   | 2.4546   | 0.9842   | 0.7721   | 0.0031   |\n",
        "| 3   | 1.0 | 1.0 | 1.0 | 9.2914   | 2.4544   | 0.9846   | 0.7722   | 0.0029   |\n",
        "| 4   | 1.0 | 5.0 | 1.0 | 9.2936   | 2.4541   | 0.9842   | 1.7464   | 0.0028   |\n",
        "| 5   | 1.0 | 0.1 | 1.0 | 9.2865   | 2.4497   | 0.9840   | 0.1253   | 0.0029   |\n",
        "| **6** | **1.0** | **0.01** | **1.0** | **9.2945** | **2.4468** | **0.9836** | **0.0136** | **0.0027** |\n",
        "| 7   | 1.5 | 0.1 | 1.0 | 9.2832   | 2.4494   | 0.9838   | 0.1253   | 0.0026   |\n",
        "\n",
        "- **Run 6** (β = 1.0, q = 0.01, η = 1.0) strikes the best balance:\n",
        "  - Sparse‐reconstruction loss drops to **0.0136** (two orders of magnitude lower)  \n",
        "  - Faithfulness ($F_{\\text{MSE}}$, $F_{\\text{MAE}}$) and orthogonality remain virtually unchanged  \n",
        "\n",
        "- Faithfulness errors vary little across settings, whereas sparsity depends strongly on $q$.  \n",
        "- Orthogonality loss stays in the $10^{-3}$ range, indicating robust concept separation.\n",
        "\n",
        "> **$F_{\\text{MSE}}$** comes from summing $(\\bar y - \\hat y)^2$ over all classes and examples.\n",
        "> \n",
        "> **$F_{\\text{MAE}}$** would come from summing $\\lvert \\bar y - \\hat y\\rvert$ instead, with the same $1/M$ averaging.\n",
        "\n",
        "## Concept Specialization {.smaller}\n",
        "\n",
        "#### Table 2 — Concept Specialization for Run 6\n",
        "\n",
        "| Digit | Top Feature | Mean Activation | Captured Pattern                       |\n",
        "|:-----:|:------------:|:---------------:|:----------------------------------------|\n",
        "| 0     | X3        | 11.757          | Closed circular stroke  \n",
        "| 1     | X1        | −10.213         | Pure vertical line  \n",
        "| 2     | X4        | 13.674          | Bottom curve with base  \n",
        "| 3     | X3        | −11.276         | Open double curve  \n",
        "| 4     | X1        | −13.920         | Vertical–horizontal junction  \n",
        "| 5     | X3        | −13.256         | Top bar plus lower curve  \n",
        "| 6     | X4        | −10.469         | Hook-like bottom curve  \n",
        "| 7     | X4        | 8.930           | Horizontal top bar  \n",
        "| 8     | X5        | 12.522          | Dual-loop structure  \n",
        "| 9     | X2        | 12.821          | Curved top with tail  \n",
        "\n",
        "- A single concept dominates each digit.  \n",
        "- **Sign** of activation encodes polarity:  \n",
        "  - X3 fires **positively** on closed loops (digit 0) but **negatively** on open curves (digits 3, 5).  \n",
        "  - X1 focuses on vertical strokes; X4 and X5 capture increasingly complex curve–line combinations.\n",
        "\n",
        "##  Concept Diversity {.smaller}\n",
        "\n",
        "#### Table 3 — Diversity versus Specialization\n",
        "\n",
        "| Feature | Primary Digit | Score   | Diversity ($\\sigma/\\mu$) | Interpretation                            |\n",
        "|:-------:|:--------------:|:-------:|:------------------------:|:------------------------------------------|\n",
        "| X1      | 4            | 13.920  | 0.504                    | Tightly tuned to vertical intersections  |\n",
        "| X2      | 9            | 12.821  | 0.543                    | Targets curved-to-vertical transitions   |\n",
        "| X3      | 5            | 13.256  | 0.794                    | Broadly active on multiple curved shapes |\n",
        "| X4      | 2            | 13.674  | 0.426                    | Highly specific to bottom-curve motifs   |\n",
        "| X5      | 8            | 12.522  | 0.658                    | Reserved for complex dual loops           |\n",
        "\n",
        "- Diversity ratio $\\sigma/\\mu$ measures how evenly a concept activates across digits.  \n",
        "- **Lower** values imply stronger focus on the primary digit (e.g., X4).  \n",
        "- **Higher** ratio (X3) reflects versatility across multiple curved shapes.\n",
        "\n",
        "## ExcitationBP Heat-maps {.smaller}\n",
        "\n",
        "![Figure 10: Variants of “4” and their top-5 concept activations](images/plot_a_first_fives_4s.png){width=60%}\n",
        "\n",
        "- Concept X3 (vertical strokes) and X5 (mid-bar) appear consistently across five “4” variants\n",
        "- Concept X1 stays negative, confirming vertical lines are handled by X3.\n",
        "\n",
        "---\n",
        "\n",
        "## ExcitationBP Heat-maps {.smaller}\n",
        "\n",
        "![Figure 11: Top-5 concepts for digits 2, 3, 6, 7, 8, 9](images/plot_b_digits_236789.png){width=60%}\n",
        "\n",
        "- Concept X4 highlights bottom curves for 2 & 6\n",
        "- Concept X2 fires on the hook of 9; X5 activates on dual loops of 8\n",
        "- Concept X1 remains mostly dormant on curved digits.\n",
        "\n",
        "---\n",
        "\n",
        "## Original Study Heat-maps {.smaller}\n",
        "\n",
        "![Figure 12: Fig 10 - Reproduced ExcitationBP from Chen et al. for digit 4 (positives and negatives)](images/studyFig10.png){width=60%}\n",
        "\n",
        "These heat-maps are less visually crisp and exhibit more off-stroke activation. Our improvement comes from a fully connected surrogate and stronger regularization.\n",
        "\n",
        "---\n",
        "\n",
        "## ExcitationBP Heat-maps {.smaller}\n",
        "\n",
        "![Figure 13: Digit 5: ExcitationBP for X1–X5](images/plot_digit_5_first5.png){width=60%}\n",
        "\n",
        "Digit 5 shows negative X3 on the open top curve and positive X4 on bottom hook; X2 weakly attends to the upper hook, revealing potential confusion with 9.\n",
        "\n",
        "---\n",
        "\n",
        "## ExcitationBP Heat-maps {.smaller}\n",
        "\n",
        "![Figure 14: Average c-MWP Heat-Maps per Digit (X1–X5)](images/average_cmwp_per_digit_wide.png){height=60%}\n",
        "\n",
        "Column-wise separation confirms each concept is specialized and orthogonal:\n",
        "\n",
        "- X1 → vertical edges (1, 4)  \n",
        "- X2 → hook (9)  \n",
        "- X3 → closed vs open curves (0 vs 3, 5)  \n",
        "- X4 → bottom curves (2, 6, 7)  \n",
        "- X5 → dual loops (8)\n",
        "\n",
        "## Discussion {.smaller}\n",
        "\n",
        "1. When we made the “q” parameter smaller, the explanation network used far fewer of the CNN’s internal features without losing accuracy in copying its outputs.\n",
        "2. Because of this, each digit only activated one or two “concepts” strongly—making it clear which part of the image mattered most.\n",
        "3. The concepts stayed very separate from each other (orthogonal), so no two concepts overlapped in what they detected.\n",
        "4. One concept (X3) even flipped its response: it lit up positively for closed loops (like the “0”) and negatively for open curves (like the “5”), showing it truly understood shape differences.\n",
        "5. The heat-maps are much sharper and cleaner than the original study’s, proving that a small, simple explanation network can still match the big CNN’s accuracy and give clear, easy-to-read visual explanations.\n",
        "\n",
        "\n",
        "## Limitations\n",
        "\n",
        "- Experiments are confined to MNIST; more complex datasets (e.g., CIFAR-10) require larger concept sets ($L\\approx10$–20) and aggressive dimensionality reduction.  \n",
        "- Faithfulness on convolution-only surrogates remains unexplored in our implementation.  \n",
        "- The revived ExcitationBP code is outdated, lacking many quality-of-life features of modern attribution libraries (e.g., Grad-CAM++).\n",
        "\n",
        "\n",
        "## Ethical Considerations {.smaller}\n",
        "\n",
        "* **Transparency & Trust**\n",
        "  By exposing a small set of human-readable “concepts,” stakeholders (clinicians, regulators, end users) can see exactly which features drove each decision. This reduces blind faith in black‐box models and builds confidence that the CNN isn’t relying on spurious cues.\n",
        "\n",
        "* **Bias Detection & Mitigation**\n",
        "  Because each concept highlights specific patterns (e.g., “vertical bar,” “curved loop”), it becomes easier to spot and correct cases where a concept might latch onto an unintended correlate—such as background textures or digit‐style biases—before deployment.\n",
        "\n",
        "* **Safety in High-Stakes Settings**\n",
        "  In domains like medical imaging or autonomous driving, knowing why a model flagged a sample (e.g., which “cell shapes” triggered a cancer warning) is crucial. The five concise concepts let experts verify that the network is focusing on medically relevant features rather than noise.\n",
        "\n",
        "* **Accountability & Compliance**\n",
        "  Regulations (EU AI Act, FDA guidelines) increasingly demand interpretability. A sparse, orthogonal explanation network satisfies these requirements by providing clear, auditable justifications for every prediction.\n",
        "\n",
        "> By combining faithfulness (matching the CNN), sparsity (only a few key features), and orthogonality (distinct explanations), XNN + SRAE represents a meaningful step toward responsible AI—balancing accuracy with human‐centered transparency, bias control, and regulatory compliance.\n",
        "\n",
        "\n",
        "\n",
        "## Further Study {.smaller}\n",
        "\n",
        "* **Refactor visualization**  \n",
        "  Modernize or replace ExcitationBP with a contemporary alternative (e.g., Grad-CAM++) to improve c-MWP averaging and injection into PyTorch hooks.  \n",
        "* **Scale-up experiments**  \n",
        "  Apply XNN + SRAE to Fashion-MNIST or CIFAR-10, increasing $L$ and benchmarking run-time versus faithfulness.  \n",
        "* **Automate concept selection**  \n",
        "  Incorporate sparsity-controlled pruning or Bayesian non-parametrics so that $L$ is chosen adaptively rather than manually.  \n",
        "* **Deploy in real time**  \n",
        "  Optimize the decoder and attribution pipeline for GPU inference so that concept heat-maps accompany live predictions in practical applications.\n",
        "\n",
        "\n",
        "\n",
        "## References {.smaller}\n",
        "\n",
        "1. Li, F., Qi, Z., Khorram, S., *et al.* (2021). *From heatmaps to structured explanations of image classifiers*. arXiv preprint arXiv:2109.06365.  \n",
        "2. Zhang, Q., Nian Wu, Y., & Zhu, S. C. (2018). *Interpretable convolutional neural networks*. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.  \n",
        "3. Doshi-Velez, F., & Kim, B. (2017). *Towards a Rigorous Science of Interpretable Machine Learning*. arXiv preprint arXiv:1702.08608.  \n",
        "4. Adebayo, J., Gilmer, J., Muelly, M., Goodfellow, I., Hardt, M., & Kim, B. (2018). *Sanity Checks for Saliency Maps*. Advances in Neural Information Processing Systems 31.  \n",
        "5. LeCun, Y., Bengio, Y., & Hinton, G. (2015). *Deep Learning*. *Nature*, 521(7553), 436-444.  \n",
        "6. Chattopadhay, A., Sarkar, A., Howlader, P., & Routray, A. (2018). *Grad-CAM++: Generalized Gradient-based Visual Explanations for Deep Convolutional Networks*. Proceedings of the IEEE Winter Conference on Applications of Computer Vision (WACV), 839-847.  \n",
        "7. Kokhlikyan, N., Miglani, V., Martin, C., Wang, E., Alsallakh, B., Reynolds, J., Melnikov, A., & Reblitz-Richardson, O. (2020). *Captum: A Unified and Generic Model Interpretability Library for PyTorch*. arXiv preprint arXiv:2009.07896.  \n",
        "8. Greydanus, S. (2019). *excitationbp: Visualizing how deep networks make decisions* (Version 0.1) [Computer software]. GitHub. https://github.com/greydanus/excitationbp  \n",
        "9. Allaire, J., & Tang, Y. (2024). tensorflow: R Interface to 'TensorFlow'. R package version 2.16.0. https://CRAN.R-project.org/package=tensorflow  \n",
        "10. Allaire, J., & Chollet, F. (2024). keras: R Interface to 'Keras'. R package version 2.15.0. https://CRAN.R-project.org/package=keras  \n",
        "11. Ushey, K., Allaire, J., & Tang, Y. (2024). reticulate: Interface to 'Python'. R package version 1.38.0. https://CRAN.R-project.org/package=reticulate  \n",
        "12. Wickham, H. (2016). ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. ISBN 978-3-319-24277-4. https://ggplot2.tidyverse.org  \n",
        "13. Wickham, H., François, R., Henry, L., Müller, K., & Vaughan, D. (2023). dplyr: A Grammar of Data Manipulation. R package version 1.1.4. https://CRAN.R-project.org/package=dplyr  \n",
        "14. Auguie, B. (2017). gridExtra: Miscellaneous Functions for \"Grid\" Graphics. R package version 2.3. https://CRAN.R-project.org/package=gridExtra  \n",
        "\n",
        "\n",
        "# Questions? {data-background-color=\"#D73F09\" .smaller}\n",
        "\n",
        "\n",
        "\n",
        "# Appendix\n",
        "\n",
        "\n",
        "## Source code (R)\n",
        "\n",
        "The complete R implementation for training and evaluating the CNN+XNN pipeline is available in the supplementary materials. Key components include:\n",
        "\n",
        "- `create_base_cnn()`: LeNet-5 architecture definition\n",
        "- `create_srae()`: SRAE explanation head construction  \n",
        "- `srae_loss()`: Composite loss function implementation\n",
        "- `train_srae()`: Custom training loop with three-loss optimization\n",
        "- `analyze_digit_features()`: Concept activation analysis\n",
        "- `evaluate_faithfulness()`: Faithfulness metric computation\n",
        "\n",
        "```markdown\n",
        "---\n",
        "title: \"XNN_SRAE_Implementation\"\n",
        "author: \"Brian Cervantes Alvarez\"\n",
        "date: \"06-02-2025\"\n",
        "format: html\n",
        "---\n",
        "```\n",
        "\n",
        "### 1. Setup and Dependencies\n",
        "\n",
        "```r\n",
        "# Install required packages if not already installed\n",
        "if (!require(tensorflow)) install.packages(\"tensorflow\")\n",
        "if (!require(keras)) install.packages(\"keras\")\n",
        "if (!require(reticulate)) install.packages(\"reticulate\")\n",
        "if (!require(ggplot2)) install.packages(\"ggplot2\")\n",
        "if (!require(dplyr)) install.packages(\"dplyr\")\n",
        "if (!require(gridExtra)) install.packages(\"gridExtra\")\n",
        "\n",
        "library(tensorflow)\n",
        "library(keras)\n",
        "library(reticulate)\n",
        "library(ggplot2)\n",
        "library(dplyr)\n",
        "library(gridExtra)\n",
        "\n",
        "# Install TensorFlow (run once)\n",
        "# install_tensorflow()\n",
        "```\n",
        "\n",
        "### 2. Load and Prepare MNIST Data\n",
        "\n",
        "```r\n",
        "# Load MNIST dataset\n",
        "mnist <- dataset_mnist()\n",
        "x_train <- mnist$train$x\n",
        "y_train <- mnist$train$y\n",
        "x_test <- mnist$test$x\n",
        "y_test <- mnist$test$y\n",
        "\n",
        "# Normalize to [0,1]\n",
        "x_train <- x_train / 255\n",
        "x_test <- x_test / 255\n",
        "\n",
        "# Reshape for CNN\n",
        "x_train <- array_reshape(x_train, c(nrow(x_train), 28, 28, 1))\n",
        "x_test <- array_reshape(x_test, c(nrow(x_test), 28, 28, 1))\n",
        "\n",
        "# Convert labels to categorical\n",
        "y_train_cat <- to_categorical(y_train, 10)\n",
        "y_test_cat <- to_categorical(y_test, 10)\n",
        "\n",
        "cat(\"Training data shape:\", dim(x_train), \"\\n\")\n",
        "cat(\"Test data shape:\", dim(x_test), \"\\n\")\n",
        "```\n",
        "\n",
        "### 3. Create and Train Base CNN\n",
        "\n",
        "```r\n",
        "create_base_cnn <- function() {\n",
        "  model <- keras_model_sequential() %>%\n",
        "    layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = 'relu',\n",
        "                  input_shape = c(28, 28, 1)) %>%\n",
        "    layer_max_pooling_2d(pool_size = c(2, 2)) %>%\n",
        "    layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = 'relu') %>%\n",
        "    layer_max_pooling_2d(pool_size = c(2, 2)) %>%\n",
        "    layer_flatten() %>%\n",
        "    layer_dense(units = 128, activation = 'relu', name = 'feature_layer') %>%\n",
        "    layer_dense(units = 10, activation = 'softmax', name = 'output_layer')\n",
        "  \n",
        "  return(model)\n",
        "}\n",
        "\n",
        "# Create and compile base CNN\n",
        "base_cnn <- create_base_cnn()\n",
        "base_cnn %>% compile(\n",
        "  optimizer = 'adam',\n",
        "  loss = 'categorical_crossentropy',\n",
        "  metrics = c('accuracy')\n",
        ")\n",
        "\n",
        "# Train the base CNN\n",
        "cat(\"Training base CNN...\\n\")\n",
        "history <- base_cnn %>% fit(\n",
        "  x_train, y_train_cat,\n",
        "  epochs = 10,\n",
        "  batch_size = 128,\n",
        "  validation_data = list(x_test, y_test_cat),\n",
        "  verbose = 1\n",
        ")\n",
        "\n",
        "\n",
        "base_accuracy <- base_cnn %>% evaluate(x_test, y_test_cat, verbose = 0)\n",
        "cat(\"Base CNN accuracy:\", base_accuracy[\"accuracy\"], \"\\n\")\n",
        "```\n",
        "\n",
        "### 4. Extract Intermediate Features (Z) and Predictions (ŷ)\n",
        "\n",
        "```r\n",
        "# Create feature extractor (Z layer)\n",
        "z_extractor <- keras_model(\n",
        "  inputs = base_cnn$input,\n",
        "  outputs = get_layer(base_cnn, 'feature_layer')$output\n",
        ")\n",
        "\n",
        "# Create logits extractor (before softmax)\n",
        "logits_model <- keras_model_sequential() %>%\n",
        "  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = 'relu',\n",
        "                input_shape = c(28, 28, 1)) %>%\n",
        "  layer_max_pooling_2d(pool_size = c(2, 2)) %>%\n",
        "  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = 'relu') %>%\n",
        "  layer_max_pooling_2d(pool_size = c(2, 2)) %>%\n",
        "  layer_flatten() %>%\n",
        "  layer_dense(units = 128, activation = 'relu') %>%\n",
        "  layer_dense(units = 10, activation = 'linear')  # No softmax for logits\n",
        "\n",
        "# Copy weights from base_cnn to logits_model\n",
        "for (i in 1:length(base_cnn$layers)) {\n",
        "  if (length(get_weights(base_cnn$layers[[i]])) > 0) {\n",
        "    if (i <= length(logits_model$layers)) {\n",
        "      set_weights(logits_model$layers[[i]], get_weights(base_cnn$layers[[i]]))\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "# Extract features and logits\n",
        "cat(\"Extracting features...\\n\")\n",
        "z_train <- predict(z_extractor, x_train)\n",
        "z_test <- predict(z_extractor, x_test)\n",
        "y_hat_train <- predict(logits_model, x_train)\n",
        "y_hat_test <- predict(logits_model, x_test)\n",
        "\n",
        "cat(\"Z features shape:\", dim(z_train), \"\\n\")\n",
        "cat(\"Y hat shape:\", dim(y_hat_train), \"\\n\")\n",
        "```\n",
        "\n",
        "### 5. Implement SRAE Model and Loss Function\n",
        "\n",
        "```r\n",
        "# Custom SRAE loss function\n",
        "# This version fixes TensorFlow/Keras compatibility issues\n",
        "\n",
        "# Simplified and fixed SRAE loss function\n",
        "srae_loss <- function(z_true, y_hat_true, e, z_reconstructed, y_pred, \n",
        "                     beta = 1.0, q = 1.0, eta = 1.0) {\n",
        "  \n",
        "  # Term 1: Faithfulness loss\n",
        "  faithfulness_loss <- tf$reduce_mean(tf$square(y_pred - y_hat_true))\n",
        "  \n",
        "  # Term 2: Sparse reconstruction loss with log penalty\n",
        "  reconstruction_errors <- tf$square(z_reconstructed - z_true)\n",
        "  reconstruction_errors_per_dim <- tf$reduce_mean(reconstruction_errors, axis = 0L)\n",
        "  sparse_reconstruction_loss <- tf$reduce_mean(\n",
        "    tf$math$log(1.0 + q * reconstruction_errors_per_dim)\n",
        "  )\n",
        "  \n",
        "  # Term 3: Orthogonality loss (simplified version)\n",
        "  e_t <- tf$transpose(e)  \n",
        "  e_feat_norm <- tf$nn$l2_normalize(e_t, axis = 1L)  \n",
        "  corr_feats <- tf$matmul(e_feat_norm, e_feat_norm, transpose_b = TRUE)\n",
        "  \n",
        "  num_xfeatures_int <- as.integer(num_xfeatures) \n",
        "  mask <- 1.0 - tf$eye(num_xfeatures_int)\n",
        "  \n",
        "  orthogonality_loss <- tf$reduce_mean(tf$square(corr_feats * mask))\n",
        "  \n",
        "  total_loss <- faithfulness_loss + beta * sparse_reconstruction_loss + eta * orthogonality_loss\n",
        "  \n",
        "  return(list(\n",
        "    total = total_loss,\n",
        "    faithfulness = faithfulness_loss,\n",
        "    sparse_reconstruction = sparse_reconstruction_loss,\n",
        "    orthogonality = orthogonality_loss\n",
        "  ))\n",
        "}\n",
        "\n",
        "# ---- 5.5 Build the SRAE network -------------------------------------------\n",
        "# z_train has shape (n_samples, 128) because the CNN’s 'feature_layer' has 128 units\n",
        "input_dim      <- ncol(z_train)      # 128\n",
        "num_xfeatures  <- 5                  # paper default (can be tuned)\n",
        "num_classes    <- 10                 # logits for MNIST digits 0‑9\n",
        "\n",
        "create_srae <- function(input_dim, num_xfeatures = 5, num_classes = 10) {\n",
        "  \n",
        "  z_input <- layer_input(shape = input_dim,  name = \"z_input\")\n",
        "  \n",
        "  # e  : low‑dimensional explanation (X‑features)\n",
        "  e_output <- z_input %>% \n",
        "    layer_dense(units = num_xfeatures,\n",
        "                activation = \"linear\",\n",
        "                name = \"explanation\")\n",
        "  \n",
        "  # ŷ  : logit prediction derived **only** from the X‑features\n",
        "  y_pred <- e_output %>% \n",
        "    layer_dense(units = num_classes,\n",
        "                activation = \"linear\",\n",
        "                name = \"prediction\")\n",
        "  \n",
        "  # ẑ  : reconstruction of the original latent vector\n",
        "  z_recon <- e_output %>% \n",
        "    layer_dense(units = input_dim,\n",
        "                activation = \"linear\",\n",
        "                name = \"z_reconstructed\")\n",
        "  \n",
        "  keras_model(inputs  = z_input,\n",
        "              outputs = list(e_output, z_recon, y_pred))\n",
        "}\n",
        "\n",
        "srae_model <- create_srae(input_dim, num_xfeatures, num_classes)\n",
        "\n",
        "```\n",
        "\n",
        "### 6. Custom Training Loop for SRAE\n",
        "\n",
        "```r\n",
        "# Custom training function for SRAE\n",
        "train_srae <- function(model, z_train, y_hat_train, z_test, y_hat_test,\n",
        "                      epochs = 100, batch_size = 256, \n",
        "                      beta = 1.0, q = 1.0, eta = 1.0) {\n",
        "  \n",
        "  # Use legacy optimizer as suggested by the warning\n",
        "  optimizer <- tf$keras$optimizers$legacy$Adam(learning_rate = 0.001)\n",
        "  \n",
        "  # Convert data to TensorFlow tensors\n",
        "  z_train_tensor <- tf$constant(z_train, dtype = tf$float32)\n",
        "  y_hat_train_tensor <- tf$constant(y_hat_train, dtype = tf$float32)\n",
        "  z_test_tensor <- tf$constant(z_test, dtype = tf$float32)\n",
        "  y_hat_test_tensor <- tf$constant(y_hat_test, dtype = tf$float32)\n",
        "  \n",
        "  # Training history\n",
        "  history <- list(\n",
        "    epoch = c(),\n",
        "    total_loss = c(),\n",
        "    faithfulness_loss = c(),\n",
        "    sparse_loss = c(),\n",
        "    orthogonality_loss = c(),\n",
        "    val_faithfulness = c()\n",
        "  )\n",
        "  \n",
        "  n_samples <- nrow(z_train)\n",
        "  n_batches <- ceiling(n_samples / batch_size)\n",
        "  \n",
        "  cat(\"Starting SRAE training...\\n\")\n",
        "  \n",
        "  # Training step function\n",
        "  train_step <- function(z_batch, y_hat_batch) {\n",
        "    with(tf$GradientTape() %as% tape, {\n",
        "      # Forward pass\n",
        "      outputs <- model(z_batch, training = TRUE)\n",
        "      e <- outputs[[1]]  # explanation\n",
        "      z_reconstructed <- outputs[[2]]  # reconstruction  \n",
        "      y_pred <- outputs[[3]]  # prediction\n",
        "      \n",
        "      # Compute loss\n",
        "      losses <- srae_loss(z_batch, y_hat_batch, \n",
        "                         e, z_reconstructed, y_pred,\n",
        "                         beta = beta, q = q, eta = eta)\n",
        "      total_loss <- losses$total\n",
        "    })\n",
        "    \n",
        "    # Compute and apply gradients\n",
        "    gradients <- tape$gradient(total_loss, model$trainable_variables)\n",
        "    optimizer$apply_gradients(purrr::transpose(list(gradients, model$trainable_variables)))\n",
        "    \n",
        "    return(losses)\n",
        "  }\n",
        "  \n",
        "  for (epoch in 1:epochs) {\n",
        "    # Shuffle training data\n",
        "    indices <- sample(1:n_samples)\n",
        "    \n",
        "    epoch_losses <- list(total = 0, faith = 0, sparse = 0, ortho = 0)\n",
        "    \n",
        "    for (batch in 1:n_batches) {\n",
        "      start_idx <- (batch - 1) * batch_size + 1\n",
        "      end_idx <- min(batch * batch_size, n_samples)\n",
        "      batch_indices <- indices[start_idx:end_idx]\n",
        "      \n",
        "      # Get batch data\n",
        "      z_batch <- tf$gather(z_train_tensor, as.integer(batch_indices - 1))\n",
        "      y_hat_batch <- tf$gather(y_hat_train_tensor, as.integer(batch_indices - 1))\n",
        "      \n",
        "      # Training step\n",
        "      batch_losses <- train_step(z_batch, y_hat_batch)\n",
        "      \n",
        "      # Accumulate losses\n",
        "      epoch_losses$total <- epoch_losses$total + as.numeric(batch_losses$total)\n",
        "      epoch_losses$faith <- epoch_losses$faith + as.numeric(batch_losses$faithfulness)\n",
        "      epoch_losses$sparse <- epoch_losses$sparse + as.numeric(batch_losses$sparse_reconstruction)\n",
        "      epoch_losses$ortho <- epoch_losses$ortho + as.numeric(batch_losses$orthogonality)\n",
        "    }\n",
        "    \n",
        "    # Average losses over batches\n",
        "    epoch_losses <- lapply(epoch_losses, function(x) x / n_batches)\n",
        "    \n",
        "    # Validation\n",
        "    val_outputs <- model(z_test_tensor, training = FALSE)\n",
        "    val_faith_loss <- tf$reduce_mean(tf$square(val_outputs[[3]] - y_hat_test_tensor))\n",
        "    \n",
        "    # Store history\n",
        "    history$epoch <- c(history$epoch, epoch)\n",
        "    history$total_loss <- c(history$total_loss, epoch_losses$total)\n",
        "    history$faithfulness_loss <- c(history$faithfulness_loss, epoch_losses$faith)\n",
        "    history$sparse_loss <- c(history$sparse_loss, epoch_losses$sparse)\n",
        "    history$orthogonality_loss <- c(history$orthogonality_loss, epoch_losses$ortho)\n",
        "    history$val_faithfulness <- c(history$val_faithfulness, as.numeric(val_faith_loss))\n",
        "    \n",
        "    # Print progress\n",
        "    if (epoch %% 10 == 0) {\n",
        "      cat(sprintf(\"Epoch %d/%d - Total Loss: %.4f, Faith: %.4f, Sparse: %.4f, Ortho: %.4f, Val Faith: %.4f\\n\",\n",
        "                  epoch, epochs, epoch_losses$total, epoch_losses$faith, \n",
        "                  epoch_losses$sparse, epoch_losses$ortho, as.numeric(val_faith_loss)))\n",
        "    }\n",
        "  }\n",
        "  \n",
        "  return(history)\n",
        "}\n",
        "\n",
        "# Train SRAE with fixed version\n",
        "training_history <- train_srae(\n",
        "  srae_model, z_train, y_hat_train, z_test, y_hat_test,\n",
        "  epochs = 50, batch_size = 256,\n",
        "  beta = 1.0, q = 1.0, eta = 1\n",
        ")\n",
        "```\n",
        "\n",
        "### 7. Analyze X-Features for Different Digits\n",
        "\n",
        "```r\n",
        "# Function to analyze X-features for specific digits\n",
        "analyze_digit_features <- function(model, digit, x_data, y_data, z_data, n_samples = 10) {\n",
        "  \n",
        "  # Get indices for specific digit\n",
        "  digit_indices <- which(y_data == digit)\n",
        "  sample_indices <- sample(digit_indices, min(n_samples, length(digit_indices)))\n",
        "  \n",
        "  # Extract features for samples\n",
        "  z_samples <- z_data[sample_indices, , drop = FALSE]\n",
        "  \n",
        "  # Get X-features\n",
        "  outputs <- model(k_constant(z_samples))\n",
        "  x_features <- as.array(outputs[[1]])  # [[1]] is the 'explanation' output\n",
        "  \n",
        "  # Compute mean and std for each X-feature\n",
        "  feature_stats <- data.frame(\n",
        "    digit = digit,\n",
        "    x_feature = paste0(\"X\", 1:5),\n",
        "    mean_activation = colMeans(x_features),\n",
        "    std_activation = apply(x_features, 2, sd),\n",
        "    stringsAsFactors = FALSE\n",
        "  )\n",
        "  \n",
        "  return(list(\n",
        "    stats = feature_stats,\n",
        "    individual_features = x_features,\n",
        "    sample_indices = sample_indices\n",
        "  ))\n",
        "}\n",
        "\n",
        "# Analyze all digits\n",
        "cat(\"Analyzing X-features for all digits...\\n\")\n",
        "all_digit_analysis <- list()\n",
        "\n",
        "for (digit in 0:9) {\n",
        "  analysis <- analyze_digit_features(srae_model, digit, x_test, y_test, z_test)\n",
        "  all_digit_analysis[[as.character(digit)]] <- analysis\n",
        "  \n",
        "  cat(sprintf(\"\\nDigit %d - Mean X-feature activations:\\n\", digit))\n",
        "  print(round(analysis$stats$mean_activation, 4))\n",
        "}\n",
        "\n",
        "# Combine results for visualization\n",
        "feature_summary <- do.call(rbind, lapply(all_digit_analysis, function(x) x$stats))\n",
        "```\n",
        "\n",
        "\n",
        "### 8. Visualize X-Feature Activation Patterns\n",
        "\n",
        "```r\n",
        "# Create X-feature activation heatmaps and bar plots\n",
        "plot_xfeature_patterns <- function(feature_summary) {\n",
        "  \n",
        "  # Heatmap of X-features by digit\n",
        "  p1 <- ggplot(feature_summary, aes(x = x_feature, y = factor(digit), fill = mean_activation)) +\n",
        "    geom_tile(color = \"white\", size = 0.5) +\n",
        "    scale_fill_gradient2(low = \"blue\", mid = \"white\", high = \"red\", \n",
        "                        midpoint = 0, name = \"Activation\") +\n",
        "    labs(title = \"X-Feature Activation Heatmap by Digit\",\n",
        "         x = \"X-Feature\", y = \"Digit\") +\n",
        "    theme_minimal() +\n",
        "    theme(\n",
        "      axis.text.x = element_text(angle = 45, hjust = 1, size = 12),\n",
        "      axis.text.y = element_text(size = 12),\n",
        "      plot.title = element_text(size = 14, hjust = 0.5),\n",
        "      legend.title = element_text(size = 12)\n",
        "    )\n",
        "  \n",
        "  # Bar plot for each digit\n",
        "  p2 <- ggplot(feature_summary, aes(x = x_feature, y = mean_activation, fill = x_feature)) +\n",
        "    geom_bar(stat = \"identity\") +\n",
        "    facet_wrap(~paste(\"Digit\", digit), scales = \"free_y\", ncol = 5) +\n",
        "    scale_fill_brewer(type = \"qual\", palette = \"Set3\") +\n",
        "    labs(title = \"X-Feature Activations by Digit\",\n",
        "         x = \"X-Feature\", y = \"Mean Activation\") +\n",
        "    theme_minimal() +\n",
        "    theme(\n",
        "      axis.text.x = element_text(angle = 45, hjust = 1),\n",
        "      legend.position = \"none\",\n",
        "      strip.text = element_text(size = 10)\n",
        "    )\n",
        "  \n",
        "  return(list(heatmap = p1, barplot = p2))\n",
        "}\n",
        "\n",
        "# Create and display the basic plots\n",
        "plots <- plot_xfeature_patterns(feature_summary)\n",
        "\n",
        "cat(\"Displaying X-Feature Heatmap...\\n\")\n",
        "print(plots$heatmap)\n",
        "\n",
        "cat(\"Displaying X-Feature Bar Plots...\\n\")\n",
        "print(plots$barplot)\n",
        "```\n",
        "\n",
        "### 9. Enhanced Heatmap Analysis\n",
        "\n",
        "```r\n",
        "# Create enhanced heatmaps with clustering\n",
        "library(reshape2)\n",
        "library(viridis)\n",
        "library(pheatmap)\n",
        "\n",
        "# Create matrix for heatmap visualization\n",
        "feature_matrix <- feature_summary %>%\n",
        "  select(digit, x_feature, mean_activation) %>%\n",
        "  reshape2::dcast(digit ~ x_feature, value.var = \"mean_activation\")\n",
        "\n",
        "rownames(feature_matrix) <- paste(\"Digit\", feature_matrix$digit)\n",
        "feature_matrix$digit <- NULL\n",
        "\n",
        "# Enhanced heatmap with clustering\n",
        "if (require(pheatmap)) {\n",
        "  cat(\"Creating enhanced heatmap with clustering...\\n\")\n",
        "  pheatmap(\n",
        "    as.matrix(feature_matrix),\n",
        "    main = \"X-Feature Activation Heatmap (Clustered)\",\n",
        "    color = viridis::viridis(100),\n",
        "    cluster_rows = TRUE,\n",
        "    cluster_cols = TRUE,\n",
        "    display_numbers = TRUE,\n",
        "    number_format = \"%.3f\",\n",
        "    fontsize_number = 8,\n",
        "    cellwidth = 40,\n",
        "    cellheight = 40\n",
        "  )\n",
        "}\n",
        "\n",
        "# Digit similarity heatmap based on X-features\n",
        "digit_correlation <- cor(t(as.matrix(feature_matrix)))\n",
        "pheatmap(\n",
        "  digit_correlation,\n",
        "  main = \"Digit Similarity Based on X-Features\",\n",
        "  color = colorRampPalette(c(\"blue\", \"white\", \"red\"))(100),\n",
        "  display_numbers = TRUE,\n",
        "  number_format = \"%.2f\",\n",
        "  fontsize_number = 8,\n",
        "  cellwidth = 30,\n",
        "  cellheight = 30\n",
        ")\n",
        "```\n",
        "\n",
        "### 10. Paper-Style X-Feature Visualization\n",
        "\n",
        "```r\n",
        "# EXCITATIONBP IMPLEMENTATION: Proper feature visualization (BETA)\n",
        "library(tensorflow)\n",
        "\n",
        "create_excitationbp_viz <- function(model, x_samples, y_samples, z_samples) {\n",
        "  \n",
        "  # Get X-features from SRAE model\n",
        "  outputs <- model(k_constant(z_samples))\n",
        "  x_features <- as.array(outputs[[1]])  # [[1]] is the 'explanation' output\n",
        "  \n",
        "  # Select digits to visualize\n",
        "  digits_to_show <- c(2, 3, 6, 7, 8, 9)\n",
        "  \n",
        "  # Select one example per digit\n",
        "  selected_examples <- list()\n",
        "  for (digit in digits_to_show) {\n",
        "    digit_indices <- which(y_samples == digit)\n",
        "    if (length(digit_indices) > 0) {\n",
        "      idx <- sample(digit_indices, 1)\n",
        "      selected_examples[[as.character(digit)]] <- list(\n",
        "        image = x_samples[idx, , , 1],\n",
        "        x_features = x_features_all[idx, ],\n",
        "        z_features = z_samples[idx, ],\n",
        "        digit = digit,\n",
        "        idx = idx\n",
        "      )\n",
        "    }\n",
        "  }\n",
        "  \n",
        "  # ExcitationBP implementation for X-features\n",
        "  excitation_bp <- function(z_input, x_feature_idx) {\n",
        "    \n",
        "    # Convert Z input to tensor\n",
        "    z_tensor <- tf$constant(matrix(z_input, nrow = 1), dtype = tf$float32)\n",
        "    \n",
        "    # Forward pass through SRAE to get X-feature activation\n",
        "    with(tf$GradientTape(persistent = TRUE) %as% tape, {\n",
        "      tape$watch(z_tensor)\n",
        "      \n",
        "      # Get model outputs\n",
        "      model_outputs <- model(z_tensor)\n",
        "      x_features <- model_outputs[[1]]  # explanation features\n",
        "      z_reconstructed <- model_outputs[[2]]  # reconstructed features\n",
        "      \n",
        "      # Target: activation of specific X-feature\n",
        "      target_activation <- x_features[1, x_feature_idx]\n",
        "    })\n",
        "    \n",
        "    # Compute gradients (standard backprop)\n",
        "    gradients <- tape$gradient(target_activation, z_tensor)\n",
        "    gradients_array <- as.array(gradients)\n",
        "    \n",
        "    # ExcitationBP: Keep only positive contributions\n",
        "    # This shows what parts of Z positively contribute to the X-feature\n",
        "    positive_gradients <- pmax(0, gradients_array)\n",
        "    \n",
        "    # Weight by the magnitude of Z features (input * gradient)\n",
        "    excitation_scores <- z_input * positive_gradients\n",
        "    \n",
        "    return(list(\n",
        "      gradients = gradients_array,\n",
        "      positive_gradients = positive_gradients,\n",
        "      excitation_scores = excitation_scores,\n",
        "      activation = as.numeric(target_activation)\n",
        "    ))\n",
        "  }\n",
        "  \n",
        "  # Convert Z-space excitation back to image space visualization\n",
        "  z_to_image_excitation <- function(z_excitation, original_image) {\n",
        "    \n",
        "    # Method 1: Create spatial mapping of Z features to image regions\n",
        "    # Each Z feature corresponds to a spatial region in the image\n",
        "    \n",
        "    excitation_image <- matrix(0, nrow = 28, ncol = 28)\n",
        "    n_z_features <- length(z_excitation)\n",
        "    \n",
        "    # Simple spatial mapping: divide image into regions for each Z feature\n",
        "    if (n_z_features >= 64) {  # If we have enough Z features\n",
        "      \n",
        "      # Create 8x8 grid mapping Z features to image regions\n",
        "      z_idx <- 1\n",
        "      for (i in seq(1, 25, by = 4)) {\n",
        "        for (j in seq(1, 25, by = 4)) {\n",
        "          if (z_idx <= n_z_features) {\n",
        "            # Map Z feature excitation to 4x4 image region\n",
        "            excitation_image[i:(i+3), j:(j+3)] <- abs(z_excitation[z_idx])\n",
        "            z_idx <- z_idx + 1\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "      \n",
        "    } else {\n",
        "      # Fallback: distribute Z features across image\n",
        "      for (z_idx in 1:min(n_z_features, 784)) {\n",
        "        i <- ((z_idx - 1) %% 28) + 1\n",
        "        j <- ((z_idx - 1) %/% 28) + 1\n",
        "        if (i <= 28 && j <= 28) {\n",
        "          excitation_image[i, j] <- abs(z_excitation[z_idx])\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    # Enhance excitation map with original image structure\n",
        "    # Only show excitation where original image has content\n",
        "    enhanced_excitation <- excitation_image * (original_image > 0.1)\n",
        "    \n",
        "    return(enhanced_excitation)\n",
        "  }\n",
        "  \n",
        "  # Generate ExcitationBP visualizations\n",
        "  cat(\"Generating ExcitationBP visualizations for X-features...\\n\")\n",
        "  \n",
        "  # Create visualization\n",
        "  par(mfrow = c(length(digits_to_show), 6),\n",
        "      mar = c(0.1, 0.1, 2, 0.1),\n",
        "      bg = \"black\")\n",
        "  \n",
        "  for (digit in digits_to_show) {\n",
        "    if (!is.null(selected_examples[[as.character(digit)]])) {\n",
        "      \n",
        "      img <- selected_examples[[as.character(digit)]]$image\n",
        "      x_vals <- selected_examples[[as.character(digit)]]$x_features\n",
        "      z_vals <- selected_examples[[as.character(digit)]]$z_features\n",
        "      \n",
        "      # Column 1: Original digit\n",
        "      image(t(img[28:1, ]), \n",
        "            col = gray.colors(256, start = 0, end = 1),\n",
        "            axes = FALSE,\n",
        "            main = \"Original Image\",\n",
        "            col.main = \"white\",\n",
        "            cex.main = 0.8)\n",
        "      \n",
        "      # Columns 2-6: ExcitationBP for each X-feature\n",
        "      for (x_idx in 1:5) {\n",
        "        x_val <- x_vals[x_idx]\n",
        "        \n",
        "        # Compute ExcitationBP for this X-feature and this specific input\n",
        "        tryCatch({\n",
        "          excitation_result <- excitation_bp(z_vals, x_idx)\n",
        "          \n",
        "          # Convert Z-space excitation to image-space visualization\n",
        "          excitation_image <- z_to_image_excitation(\n",
        "            excitation_result$excitation_scores, \n",
        "            img\n",
        "          )\n",
        "          \n",
        "          # Enhance visualization based on activation strength\n",
        "          if (x_val >= 0) {\n",
        "            # Positive activation: show excitation as bright regions\n",
        "            final_pattern <- excitation_image * (abs(x_val) / max(abs(x_features_all[, x_idx])))\n",
        "          } else {\n",
        "            # Negative activation: show as inverted excitation\n",
        "            final_pattern <- 1 - (excitation_image * (abs(x_val) / max(abs(x_features_all[, x_idx]))))\n",
        "          }\n",
        "          \n",
        "          # Normalize for display\n",
        "          if (max(final_pattern) > min(final_pattern)) {\n",
        "            final_pattern <- (final_pattern - min(final_pattern)) / \n",
        "                            (max(final_pattern) - min(final_pattern))\n",
        "          } else {\n",
        "            final_pattern <- matrix(0.5, nrow = 28, ncol = 28)\n",
        "          }\n",
        "          \n",
        "        }, error = function(e) {\n",
        "          cat(paste(\"ExcitationBP error for X-feature\", x_idx, \":\", e$message, \"\\n\"))\n",
        "          # Fallback pattern\n",
        "          final_pattern <- matrix(0.5, nrow = 28, ncol = 28)\n",
        "        })\n",
        "        \n",
        "        # Display ExcitationBP result\n",
        "        image(t(final_pattern[28:1, ]), \n",
        "              col = gray.colors(256, start = 0, end = 1),\n",
        "              axes = FALSE,\n",
        "              main = paste0(\"X\", x_idx, \": \", sprintf(\"%.4f\", x_val)),\n",
        "              col.main = \"white\",\n",
        "              cex.main = 0.7)\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "  \n",
        "  # Reset background\n",
        "  par(bg = \"white\")\n",
        "  \n",
        "  return(list(examples = selected_examples))\n",
        "}\n",
        "\n",
        "# Simpler ExcitationBP using correlation method\n",
        "create_correlation_excitationbp <- function(model, x_samples, y_samples, z_samples) {\n",
        "  \n",
        "  # This approach computes correlation between each X-feature and input pixels\n",
        "  # across the entire dataset - a simplified form of ExcitationBP\n",
        "  \n",
        "  outputs <- model(k_constant(z_samples))\n",
        "  x_features_all <- as.array(outputs[[1]])\n",
        "  \n",
        "  # Compute correlation maps for each X-feature\n",
        "  cat(\"Computing X-feature correlation maps (simplified ExcitationBP)...\\n\")\n",
        "  correlation_maps <- list()\n",
        "  \n",
        "  for (x_idx in 1:5) {\n",
        "    correlation_map <- matrix(0, nrow = 28, ncol = 28)\n",
        "    x_activations <- x_features_all[, x_idx]\n",
        "    \n",
        "    # Use subset for efficiency\n",
        "    n_samples <- min(2000, nrow(x_samples))\n",
        "    sample_indices <- sample(1:nrow(x_samples), n_samples)\n",
        "    \n",
        "    for (i in 1:28) {\n",
        "      for (j in 1:28) {\n",
        "        pixel_values <- x_samples[sample_indices, i, j, 1]\n",
        "        feature_values <- x_activations[sample_indices]\n",
        "        \n",
        "        if (sd(pixel_values) > 0 && sd(feature_values) > 0) {\n",
        "          correlation_map[i, j] <- cor(pixel_values, feature_values)\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    correlation_maps[[x_idx]] <- correlation_map\n",
        "    cat(paste(\"Computed correlation map for X-feature\", x_idx, \"\\n\"))\n",
        "  }\n",
        "  \n",
        "  # Now create input-specific visualizations\n",
        "  digits_to_show <- c(2, 3, 6, 7, 8, 9)\n",
        "  \n",
        "  selected_examples <- list()\n",
        "  for (digit in digits_to_show) {\n",
        "    digit_indices <- which(y_samples == digit)\n",
        "    if (length(digit_indices) > 0) {\n",
        "      idx <- sample(digit_indices, 1)\n",
        "      selected_examples[[as.character(digit)]] <- list(\n",
        "        image = x_samples[idx, , , 1],\n",
        "        x_features = x_features_all[idx, ],\n",
        "        digit = digit\n",
        "      )\n",
        "    }\n",
        "  }\n",
        "  \n",
        "  # Create visualization\n",
        "  par(mfrow = c(length(digits_to_show), 6),\n",
        "      mar = c(0.1, 0.1, 2, 0.1),\n",
        "      bg = \"black\")\n",
        "  \n",
        "  for (digit in digits_to_show) {\n",
        "    if (!is.null(selected_examples[[as.character(digit)]])) {\n",
        "      \n",
        "      img <- selected_examples[[as.character(digit)]]$image\n",
        "      x_vals <- selected_examples[[as.character(digit)]]$x_features\n",
        "      \n",
        "      # Original image\n",
        "      image(t(img[28:1, ]), \n",
        "            col = gray.colors(256, start = 0, end = 1),\n",
        "            axes = FALSE,\n",
        "            main = \"Original Image\",\n",
        "            col.main = \"white\",\n",
        "            cex.main = 0.8)\n",
        "      \n",
        "      # ExcitationBP-style visualizations\n",
        "      for (x_idx in 1:5) {\n",
        "        x_val <- x_vals[x_idx]\n",
        "        \n",
        "        # Get correlation map for this X-feature\n",
        "        correlation_map <- correlation_maps[[x_idx]]\n",
        "        \n",
        "        # Weight by actual activation and input image\n",
        "        input_weighted_excitation <- correlation_map * img * sign(x_val)\n",
        "        \n",
        "        # Scale by activation strength\n",
        "        max_activation <- max(abs(x_features_all[, x_idx]))\n",
        "        if (max_activation > 0) {\n",
        "          activation_strength <- abs(x_val) / max_activation\n",
        "          final_pattern <- input_weighted_excitation * activation_strength\n",
        "        } else {\n",
        "          final_pattern <- input_weighted_excitation * 0.1\n",
        "        }\n",
        "        \n",
        "        # Normalize for display\n",
        "        if (max(final_pattern) > min(final_pattern)) {\n",
        "          final_pattern <- (final_pattern - min(final_pattern)) / \n",
        "                          (max(final_pattern) - min(final_pattern))\n",
        "        } else {\n",
        "          final_pattern <- matrix(0.5, nrow = 28, ncol = 28)\n",
        "        }\n",
        "        \n",
        "        # Display\n",
        "        image(t(final_pattern[28:1, ]), \n",
        "              col = gray.colors(256, start = 0, end = 1),\n",
        "              axes = FALSE,\n",
        "              main = paste0(\"X\", x_idx, \": \", sprintf(\"%.4f\", x_val)),\n",
        "              col.main = \"white\",\n",
        "              cex.main = 0.7)\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "  \n",
        "  par(bg = \"white\")\n",
        "  return(list(examples = selected_examples, correlation_maps = correlation_maps))\n",
        "}\n",
        "\n",
        "# Run ExcitationBP visualization\n",
        "cat(\"Creating ExcitationBP-based X-feature visualization...\\n\")\n",
        "\n",
        "# Try the correlation-based method first (more stable)\n",
        "x_viz <- x_test\n",
        "y_viz <- y_test\n",
        "z_viz <- z_test\n",
        "paper_viz_results <- create_correlation_excitationbp(srae_model, x_viz, y_viz, z_viz)\n",
        "\n",
        "cat(\"ExcitationBP visualization complete!\\n\")\n",
        "cat(\"Now showing input-specific excitation patterns for each X-feature\\n\")\n",
        "```\n",
        "\n",
        "### 11. Analysis and Interpretation\n",
        "\n",
        "```r\n",
        "# Analyze the X-feature patterns for insights\n",
        "analyze_xfeature_insights <- function(paper_viz_results, feature_summary) {\n",
        "  \n",
        "  cat(\"\\n=== X-FEATURE ANALYSIS INSIGHTS ===\\n\")\n",
        "  \n",
        "  # Find dominant X-features for each digit\n",
        "  cat(\"\\nDominant X-features by digit:\\n\")\n",
        "  for (digit in 0:9) {\n",
        "    digit_features <- feature_summary[feature_summary$digit == digit, ]\n",
        "    if (nrow(digit_features) > 0) {\n",
        "      top_feature_idx <- which.max(abs(digit_features$mean_activation))\n",
        "      top_feature <- digit_features$x_feature[top_feature_idx]\n",
        "      top_value <- digit_features$mean_activation[top_feature_idx]\n",
        "      \n",
        "      cat(sprintf(\"Digit %d: %s (%.3f)\\n\", digit, top_feature, top_value))\n",
        "    }\n",
        "  }\n",
        "  \n",
        "  # Analyze X-feature specialization\n",
        "  cat(\"\\nX-feature specialization analysis:\\n\")\n",
        "  for (x_feat in paste0(\"X\", 1:5)) {\n",
        "    feat_data <- feature_summary[feature_summary$x_feature == x_feat, ]\n",
        "    max_digit <- feat_data$digit[which.max(abs(feat_data$mean_activation))]\n",
        "    max_value <- max(abs(feat_data$mean_activation))\n",
        "    \n",
        "    cat(sprintf(\"%s: Most activated by Digit %d (%.3f)\\n\", x_feat, max_digit, max_value))\n",
        "  }\n",
        "  \n",
        "  # Compute feature diversity (how specialized each X-feature is)\n",
        "  cat(\"\\nX-feature diversity (lower = more specialized):\\n\")\n",
        "  for (x_feat in paste0(\"X\", 1:5)) {\n",
        "    feat_values <- feature_summary[feature_summary$x_feature == x_feat, \"mean_activation\"]\n",
        "    diversity <- sd(abs(feat_values)) / mean(abs(feat_values))\n",
        "    cat(sprintf(\"%s: %.3f\\n\", x_feat, diversity))\n",
        "  }\n",
        "  \n",
        "  return(invisible())\n",
        "}\n",
        "\n",
        "# Run the analysis\n",
        "analyze_xfeature_insights(paper_viz_results, feature_summary)\n",
        "\n",
        "cat(\"\\n=== VISUALIZATION SUMMARY ===\\n\")\n",
        "cat(\"Generated visualizations:\\n\")\n",
        "cat(\"1. X-Feature activation heatmaps (Section 8)\\n\")\n",
        "cat(\"2. Enhanced clustered heatmaps (Section 9)\\n\") \n",
        "cat(\"3. Paper-style X-feature patterns (Section 10)\\n\")\n",
        "cat(\"4. Analysis and insights (Section 11)\\n\")\n",
        "```\n",
        "\n",
        "\n",
        "### 12. Evaluate Model Performance\n",
        "\n",
        "```r\n",
        "# Evaluate faithfulness (how well SRAE predicts original CNN output)\n",
        "evaluate_faithfulness <- function(model, z_test, y_hat_test) {\n",
        "  \n",
        "  outputs <- model(k_constant(z_test))\n",
        "  predicted_logits <- as.array(outputs$prediction)\n",
        "  \n",
        "  # Mean squared error\n",
        "  mse <- mean((predicted_logits - y_hat_test)^2)\n",
        "  \n",
        "  # Classification accuracy\n",
        "  predicted_classes <- max.col(predicted_logits) - 1\n",
        "  original_classes <- max.col(y_hat_test) - 1\n",
        "  accuracy <- mean(predicted_classes == original_classes)\n",
        "  \n",
        "  cat(\"SRAE Evaluation:\\n\")\n",
        "  cat(\"Faithfulness (MSE):\", round(mse, 6), \"\\n\")\n",
        "  cat(\"Classification Accuracy:\", round(accuracy, 4), \"\\n\")\n",
        "  \n",
        "  return(list(mse = mse, accuracy = accuracy))\n",
        "}\n",
        "\n",
        "faithfulness_results <- evaluate_faithfulness(srae_model, z_test, y_hat_test)\n",
        "\n",
        "# Print summary insights\n",
        "cat(\"\\n=== SUMMARY INSIGHTS ===\\n\")\n",
        "cat(\"The X-features represent different visual patterns:\\n\")\n",
        "\n",
        "# Find which X-features are most important for each digit\n",
        "for (digit in 0:9) {\n",
        "  features <- all_digit_analysis[[as.character(digit)]]$stats$mean_activation\n",
        "  top_feature <- which.max(abs(features))\n",
        "  cat(sprintf(\"Digit %d: Most activated by X%d (%.4f)\\n\", \n",
        "              digit, top_feature, features[top_feature]))\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## Source code (Python) - ExcitationBP\n",
        "\n",
        "\n",
        "### plot_multiple_cMWP.py"
      ],
      "id": "b0d6a463"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#!/usr/bin/env python3\n",
        "# plot_multiple_cMWP.py\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from excitationbp import ExcitationBP\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 1) Load the two SavedModels exported from R:\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "cnn_path  = os.path.join(\"saved_models\", \"base_cnn_for_excitebp\")\n",
        "srae_path = os.path.join(\"saved_models\", \"srae_explainer\")\n",
        "\n",
        "print(\"Loading base CNN from:\", cnn_path)\n",
        "py_cnn = tf.keras.models.load_model(cnn_path)\n",
        "\n",
        "print(\"Loading SRAE explainer from:\", srae_path)\n",
        "py_srae = tf.keras.models.load_model(srae_path, compile=False)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 2) Print layer names so we can verify how to stitch:\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "print(\"\\n--- CNN layers ---\")\n",
        "for layer in py_cnn.layers:\n",
        "    print(f\"{layer.name:30s}  {layer.output_shape}\")\n",
        "\n",
        "print(\"\\n--- SRAE layers ---\")\n",
        "for layer in py_srae.layers:\n",
        "    print(f\"{layer.name:30s}  {layer.input_shape} → {layer.output_shape}\")\n",
        "\n",
        "# Sanity check:\n",
        "assert py_cnn.get_layer(\"feature_layer\").output_shape[-1] == 128\n",
        "assert py_srae.get_layer(\"explanation\").input_shape[-1] == 128\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 3) “Stitch” the SRAE’s ‘explanation’ Dense onto the CNN:\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "#\n",
        "#    We copy just the weights of the original `explanation` layer\n",
        "#    so that our fused model uses a true Dense, not a Lambda. This\n",
        "#    ensures `ebp.excite(…, \"explanation\", i)` will find exactly\n",
        "#    one layer named \"explanation\" to hook into.\n",
        "#\n",
        "orig_ex_layer = py_srae.get_layer(\"explanation\")\n",
        "new_ex_layer = tf.keras.layers.Dense(\n",
        "    units      = orig_ex_layer.units,\n",
        "    activation = orig_ex_layer.activation,\n",
        "    use_bias   = orig_ex_layer.use_bias,\n",
        "    name       = \"explanation\"\n",
        ")\n",
        "# Build & transfer weights:\n",
        "new_ex_layer.build((None, 128))\n",
        "new_ex_layer.set_weights(orig_ex_layer.get_weights())\n",
        "\n",
        "# Build fused model: image → base_CNN → feature_layer(128) → new_explanation(5)\n",
        "image_input    = py_cnn.input                                   # shape=(None,28,28,1)\n",
        "feature_tensor = py_cnn.get_layer(\"feature_layer\").output       # shape=(None,128)\n",
        "explanation_tensor = new_ex_layer(feature_tensor)               # shape=(None,5)\n",
        "\n",
        "full_model = tf.keras.Model(\n",
        "    inputs  = image_input,\n",
        "    outputs = explanation_tensor,\n",
        "    name    = \"cnn_srae_fused\"\n",
        ")\n",
        "print(\"\\nFull fused model summary:\")\n",
        "full_model.summary()\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 4) Instantiate ExcitationBP on the fused model:\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "ebp = ExcitationBP(full_model)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 5) Load MNIST and select examples:\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "x_test = np.expand_dims(x_test, axis=-1)  # shape = (N, 28, 28, 1)\n",
        "\n",
        "# (a) first five examples of “4”\n",
        "four_indices = np.where(y_test == 4)[0]\n",
        "sel4 = four_indices[:5]\n",
        "\n",
        "# (b) one example each of [2,3,6,7,8,9]\n",
        "digits_b = [2, 3, 6, 7, 8, 9]\n",
        "sel_b = []\n",
        "for d in digits_b:\n",
        "    idx_list = np.where(y_test == d)[0]\n",
        "    sel_b.append(idx_list[0])\n",
        "\n",
        "print(\"\\nPlot (a): indices of the first five 4’s:\", sel4.tolist())\n",
        "print(\"Plot (b): indices of one example each of [2,3,6,7,8,9]:\", sel_b)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 6) Helper to compute all c-MWP maps for a single index:\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "def compute_cmwp_for_index(idx):\n",
        "    \"\"\"\n",
        "    Given a single MNIST index `idx`, returns:\n",
        "      - orig_img:  (28×28) array\n",
        "      - x_feats:   length‐5 array of X‐feature activations\n",
        "      - cmwp_maps: list of five (28×28) heatmaps (one per X‐feature)\n",
        "    \"\"\"\n",
        "    # 1) Grab the (1,28,28,1) input, run it through the fused model\n",
        "    img = x_test[idx : idx + 1, ...]                          # shape=(1,28,28,1)\n",
        "    x_feats = full_model.predict(img, verbose=0).reshape(-1)  # shape=(5,)\n",
        "\n",
        "    # 2) Compute c-MWP for each X‐feature i in [0..4]\n",
        "    cmwp_maps = []\n",
        "    for i in range(len(x_feats)):\n",
        "        hm_tensor = ebp.excite(img, \"explanation\", i)\n",
        "        hm = hm_tensor.numpy().squeeze()  # shape=(28,28)\n",
        "        cmwp_maps.append(hm)\n",
        "    return img.squeeze(), x_feats, cmwp_maps\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 7) Plot (a): “first five 4’s” in a 5×6 grid:\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "num_xfeat = 5\n",
        "n_rows_a  = len(sel4)\n",
        "n_cols    = num_xfeat + 1  # one column for the original, plus five heatmaps\n",
        "\n",
        "plt.figure(figsize=(n_cols * 2, n_rows_a * 2))\n",
        "plt.suptitle(\"Plot (a): c-MWP heatmaps for the first five 4’s\", fontsize=16, y=0.92)\n",
        "\n",
        "for row_i, idx in enumerate(sel4):\n",
        "    orig_img, x_feats, maps = compute_cmwp_for_index(idx)\n",
        "\n",
        "    # ─── Column 0: Original MNIST “4” ──────────────────────\n",
        "    ax = plt.subplot(n_rows_a, n_cols, row_i * n_cols + 1)\n",
        "    # Just show orig_img directly (no [::-1,:], no origin='lower'):\n",
        "    plt.imshow(orig_img, cmap=\"gray\", vmin=0, vmax=1)\n",
        "    ax.set_xticks([]); ax.set_yticks([])\n",
        "    ax.set_title(f\"Original Digit\", color=\"white\", backgroundcolor=\"black\", fontsize=10)\n",
        "\n",
        "    # ─── Columns 1..5: c-MWP heatmaps for X₁…X₅ ─────────────\n",
        "    for feat_i in range(num_xfeat):\n",
        "        hm = maps[feat_i]\n",
        "        title_str = f\"X{feat_i+1}: {x_feats[feat_i]:.4f}\"\n",
        "\n",
        "        ax = plt.subplot(n_rows_a, n_cols, row_i * n_cols + (feat_i + 2))\n",
        "        # Show hm as‐is (no [::-1,:], no origin='lower'):\n",
        "        plt.imshow(hm, cmap=\"viridis\", vmin=hm.min(), vmax=hm.max())\n",
        "        ax.set_xticks([]); ax.set_yticks([])\n",
        "        ax.set_title(title_str, color=\"white\", backgroundcolor=\"black\", fontsize=8)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.90])\n",
        "os.makedirs(\"figures\", exist_ok=True)\n",
        "out_a = os.path.join(\"figures\", \"plot_a_first_fives_4s.png\")\n",
        "plt.savefig(out_a, dpi=150, bbox_inches=\"tight\")\n",
        "print(f\"\\nSaved Plot (a) to {out_a}\")\n",
        "plt.close()\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 8) Plot (b): digits [2,3,6,7,8,9] in a 6×6 grid:\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "n_rows_b = len(sel_b)\n",
        "plt.figure(figsize=(n_cols * 2, n_rows_b * 2))\n",
        "plt.suptitle(\"Plot (b): c-MWP heatmaps for digits [2,3,6,7,8,9]\", fontsize=16, y=0.92)\n",
        "\n",
        "for row_i, idx in enumerate(sel_b):\n",
        "    orig_img, x_feats, maps = compute_cmwp_for_index(idx)\n",
        "    digit_label = int(y_test[idx])\n",
        "\n",
        "    # ─── Column 0: Original MNIST digit ─────────────────────\n",
        "    ax = plt.subplot(n_rows_b, n_cols, row_i * n_cols + 1)\n",
        "    plt.imshow(orig_img, cmap=\"gray\", vmin=0, vmax=1)  # no flips\n",
        "    ax.set_xticks([]); ax.set_yticks([])\n",
        "    ax.set_title(f\"Idx={idx}, {digit_label}\", color=\"white\", backgroundcolor=\"black\", fontsize=10)\n",
        "\n",
        "    # ─── Columns 1..5: c-MWP heatmaps for X₁…X₅ ─────────────\n",
        "    for feat_i in range(num_xfeat):\n",
        "        hm = maps[feat_i]\n",
        "        title_str = f\"X{feat_i+1}: {x_feats[feat_i]:.4f}\"\n",
        "\n",
        "        ax = plt.subplot(n_rows_b, n_cols, row_i * n_cols + (feat_i + 2))\n",
        "        plt.imshow(hm, cmap=\"viridis\", vmin=hm.min(), vmax=hm.max())\n",
        "        ax.set_xticks([]); ax.set_yticks([])\n",
        "        ax.set_title(title_str, color=\"white\", backgroundcolor=\"black\", fontsize=8)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.90])\n",
        "out_b = os.path.join(\"figures\", \"plot_b_digits_236789.png\")\n",
        "plt.savefig(out_b, dpi=150, bbox_inches=\"tight\")\n",
        "print(f\"\\nSaved Plot (b) to {out_b}\")\n",
        "plt.close()\n",
        "\n",
        "print(\"\\nAll plots generated.\\n\")"
      ],
      "id": "cc196475",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### plot_each_digit_first5.py"
      ],
      "id": "1f9c3943"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#!/usr/bin/env python3\n",
        "# plot_each_digit_first5.py\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from excitationbp import ExcitationBP\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 1) Load the CNN‐up‐to‐Dense(128) and SRAE “explanation” models, then stitch them\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "cnn_path  = os.path.join(\"saved_models\", \"base_cnn_for_excitebp\")\n",
        "srae_path = os.path.join(\"saved_models\", \"srae_explainer\")\n",
        "\n",
        "print(\"Loading base CNN from:\", cnn_path)\n",
        "py_cnn = tf.keras.models.load_model(cnn_path)\n",
        "\n",
        "print(\"Loading SRAE explainer from:\", srae_path)\n",
        "py_srae = tf.keras.models.load_model(srae_path, compile=False)\n",
        "\n",
        "# Verify layer names\n",
        "print(\"\\n--- CNN layers ---\")\n",
        "for layer in py_cnn.layers:\n",
        "    print(f\"{layer.name:30s}  {layer.output_shape}\")\n",
        "print(\"\\n--- SRAE layers ---\")\n",
        "for layer in py_srae.layers:\n",
        "    print(f\"{layer.name:30s}  {layer.input_shape} → {layer.output_shape}\")\n",
        "\n",
        "# We expect py_cnn.get_layer(\"feature_layer\").output_shape[-1] == 128\n",
        "assert py_cnn.get_layer(\"feature_layer\").output_shape[-1] == 128\n",
        "# We expect py_srae.get_layer(\"explanation\").input_shape[-1] == 128\n",
        "assert py_srae.get_layer(\"explanation\").input_shape[-1] == 128\n",
        "\n",
        "# Build a brand‐new Dense( “explanation” ) layer so that\n",
        "# it has a real layer name “explanation” (not a Lambda).\n",
        "orig_expl = py_srae.get_layer(\"explanation\")\n",
        "new_expl  = tf.keras.layers.Dense(\n",
        "    units      = orig_expl.units,\n",
        "    activation = orig_expl.activation,\n",
        "    use_bias   = orig_expl.use_bias,\n",
        "    name       = \"explanation\"\n",
        ")\n",
        "new_expl.build((None, 128))\n",
        "new_expl.set_weights(orig_expl.get_weights())\n",
        "\n",
        "# Stitch it onto the CNN’s “feature_layer”\n",
        "image_input       = py_cnn.input\n",
        "feature_tensor    = py_cnn.get_layer(\"feature_layer\").output\n",
        "explanation_tensor = new_expl(feature_tensor)\n",
        "\n",
        "full_model = tf.keras.Model(\n",
        "    inputs  = image_input,\n",
        "    outputs = explanation_tensor,\n",
        "    name    = \"cnn_srae_fused\"\n",
        ")\n",
        "print(\"\\nFull fused model summary:\")\n",
        "full_model.summary()\n",
        "\n",
        "# Wrap with ExcitationBP\n",
        "ebp = ExcitationBP(full_model)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 2) Load MNIST test set\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "(_, _), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "x_test = np.expand_dims(x_test, axis=-1)  # shape = (N,28,28,1)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 3) For each digit in [0,1,2,3,5,6,7,8,9], pick the first five indices:\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "digits_to_plot = [0, 1, 2, 3, 5, 6, 7, 8, 9]\n",
        "first5_dict = {}\n",
        "\n",
        "for d in digits_to_plot:\n",
        "    idxs = np.where(y_test == d)[0]\n",
        "    if len(idxs) < 5:\n",
        "        raise RuntimeError(f\"Found fewer than 5 examples of digit {d} in test set!\")\n",
        "    first5_dict[d] = idxs[:5]\n",
        "\n",
        "print(\"\\nWill create one figure per digit (first 5 examples each).\")\n",
        "for d in digits_to_plot:\n",
        "    print(f\"  Digit {d}: indices {first5_dict[d].tolist()}\")\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 4) Helper: given a single test‐index, compute:\n",
        "#      (a) orig_img   = 28×28 array\n",
        "#      (b) x_feats    = length‐5 np.ndarray of explanation activations\n",
        "#      (c) cmwp_maps  = list of five (28×28) c-MWP heatmaps\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "def compute_cmwp_for_index(idx):\n",
        "    # “inp” shape = (1,28,28,1)\n",
        "    inp = x_test[idx : idx + 1, ...]\n",
        "    # (1×5) → reshape to (5,) for X₁…X₅ activations\n",
        "    x_feats = full_model.predict(inp, verbose=0).reshape(-1)\n",
        "    cmwp_maps = []\n",
        "    for i in range(len(x_feats)):\n",
        "        hm_t  = ebp.excite(inp, \"explanation\", i)\n",
        "        hm_np = hm_t.numpy().squeeze()  # → (28,28)\n",
        "        cmwp_maps.append(hm_np)\n",
        "    orig_img = inp.squeeze()  # (28,28)\n",
        "    return orig_img, x_feats, cmwp_maps\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 5) Loop over each digit, build a separate figure, and save it:\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "os.makedirs(\"figures\", exist_ok=True)\n",
        "\n",
        "for d in digits_to_plot:\n",
        "    five_indices = first5_dict[d]\n",
        "\n",
        "    # Build a new figure: 5 rows × 6 columns\n",
        "    plt.figure(figsize=(6 * 2.0, 5 * 2.0))\n",
        "    plt.suptitle(\n",
        "        f\"c-MWP heatmaps for the first five '{d}'s\",\n",
        "        fontsize = 16,\n",
        "        y        = 0.92\n",
        "    )\n",
        "\n",
        "    # For each of the five examples of digit d:\n",
        "    for row_i, idx in enumerate(five_indices):\n",
        "        orig_img, x_feats, cmwp_maps = compute_cmwp_for_index(idx)\n",
        "\n",
        "        # ─── Column 0 (Original Digit) ─────────────────────────\n",
        "        ax = plt.subplot(5, 6, row_i * 6 + 1)\n",
        "        plt.imshow(orig_img, cmap=\"gray\", vmin=0, vmax=1)\n",
        "        ax.set_xticks([]); ax.set_yticks([])\n",
        "        # Title = “Original Digit” (no index)\n",
        "        ax.set_title(\"Original Digit\",\n",
        "                     color=\"white\",\n",
        "                     backgroundcolor=\"black\",\n",
        "                     fontsize=9)\n",
        "\n",
        "        # ─── Columns 1…5 (c-MWP for X₁…X₅) ───────────────────────\n",
        "        for feat_i in range(5):\n",
        "            hm   = cmwp_maps[feat_i]\n",
        "            val  = x_feats[feat_i]\n",
        "            title_str = f\"X{feat_i+1}: {val:.3f}\"\n",
        "\n",
        "            ax = plt.subplot(5, 6, row_i * 6 + (feat_i + 2))\n",
        "            plt.imshow(hm, cmap=\"viridis\", vmin=hm.min(), vmax=hm.max())\n",
        "            ax.set_xticks([]); ax.set_yticks([])\n",
        "            ax.set_title(title_str,\n",
        "                         color=\"white\",\n",
        "                         backgroundcolor=\"black\",\n",
        "                         fontsize=7)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.90])\n",
        "    save_name = f\"plot_digit_{d}_first5.png\"\n",
        "    save_path = os.path.join(\"figures\", save_name)\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
        "    print(f\"Saved figure for digit {d} → {save_path}\")\n",
        "    plt.close()\n",
        "\n",
        "print(\"\\nAll done. You should now have nine separate PNGs in ./figures/:\\n\"\n",
        "      \"  plot_digit_0_first5.png\\n\"\n",
        "      \"  plot_digit_1_first5.png\\n\"\n",
        "      \"  plot_digit_2_first5.png\\n\"\n",
        "      \"  plot_digit_3_first5.png\\n\"\n",
        "      \"  plot_digit_5_first5.png\\n\"\n",
        "      \"  plot_digit_6_first5.png\\n\"\n",
        "      \"  plot_digit_7_first5.png\\n\"\n",
        "      \"  plot_digit_8_first5.png\\n\"\n",
        "      \"  plot_digit_9_first5.png\\n\")"
      ],
      "id": "f7b2da04",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### average_cmwp_per_digit.py"
      ],
      "id": "db503af8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#!/usr/bin/env python3\n",
        "# average_cmwp_per_digit.py\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from excitationbp import ExcitationBP\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 1) Load & stitch the CNN + SRAE so that “explanation” is a real Dense layer.\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "cnn_path  = os.path.join(\"saved_models\", \"base_cnn_for_excitebp\")\n",
        "srae_path = os.path.join(\"saved_models\", \"srae_explainer\")\n",
        "\n",
        "print(\"Loading base CNN from:\", cnn_path)\n",
        "py_cnn = tf.keras.models.load_model(cnn_path)\n",
        "\n",
        "print(\"Loading SRAE explainer from:\", srae_path)\n",
        "py_srae = tf.keras.models.load_model(srae_path, compile=False)\n",
        "\n",
        "# If “explanation” was exported as a Lambda, replace it with a real Dense\n",
        "orig_expl = py_srae.get_layer(\"explanation\")\n",
        "new_expl = tf.keras.layers.Dense(\n",
        "    units      = orig_expl.units,\n",
        "    activation = orig_expl.activation,\n",
        "    use_bias   = orig_expl.use_bias,\n",
        "    name       = \"explanation\"\n",
        ")\n",
        "# Build & copy weights\n",
        "new_expl.build(input_shape=(None, orig_expl.input_shape[-1]))\n",
        "new_expl.set_weights(orig_expl.get_weights())\n",
        "\n",
        "# Re‐stitch onto the CNN’s feature_layer output\n",
        "image_input        = py_cnn.input                              # (None,28,28,1)\n",
        "feature_tensor     = py_cnn.get_layer(\"feature_layer\").output  # (None,128)\n",
        "explanation_tensor = new_expl(feature_tensor)                  # (None,5)\n",
        "\n",
        "full_model = tf.keras.Model(\n",
        "    inputs  = image_input,\n",
        "    outputs = explanation_tensor,\n",
        "    name    = \"cnn_srae_fused\"\n",
        ")\n",
        "\n",
        "print(\"\\nFull fused model summary:\")\n",
        "full_model.summary()\n",
        "\n",
        "# Wrap in ExcitationBP\n",
        "ebp = ExcitationBP(full_model)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 2) Load MNIST test set and group indices by digit\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "(_, _), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "x_test = np.expand_dims(x_test, axis=-1)   # shape = (10000,28,28,1)\n",
        "y_test = y_test.astype(\"int\")\n",
        "\n",
        "N_total = x_test.shape[0]\n",
        "print(f\"\\nLoaded MNIST test set: {N_total} images.\")\n",
        "\n",
        "digit_indices = {d: np.where(y_test == d)[0] for d in range(10)}\n",
        "for d in range(10):\n",
        "    print(f\"  → Digit {d}: {len(digit_indices[d])} examples\")\n",
        "\n",
        "# Prepare accumulators: avg_maps[digit][feature_index] = (28×28) float64\n",
        "avg_maps = [\n",
        "    [np.zeros((28, 28), dtype=np.float64) for _ in range(5)]\n",
        "    for _ in range(10)\n",
        "]\n",
        "counts = [len(digit_indices[d]) for d in range(10)]\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 3) Accumulate c-MWP heatmaps per (digit, feature), then divide by count\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "print(\"\\nAccumulating c-MWP maps, grouped by digit and feature…\")\n",
        "for d in range(10):\n",
        "    idxs = digit_indices[d]\n",
        "    c = len(idxs)\n",
        "    print(f\"\\n  → Digit {d}: {c} images…\")\n",
        "    for (k, n) in enumerate(idxs):\n",
        "        inp = x_test[n : n + 1, ...]  # shape = (1,28,28,1)\n",
        "        # Compute c-MWP for each of the 5 X-features\n",
        "        for i in range(5):\n",
        "            hm = ebp.excite(inp, \"explanation\", i).numpy().squeeze()\n",
        "            avg_maps[d][i] += hm\n",
        "\n",
        "        if (k + 1) % 2000 == 0 or (k + 1) == c:\n",
        "            print(f\"      • processed {k+1}/{c} for digit {d}\")\n",
        "\n",
        "    # Convert sums into averages\n",
        "    for i in range(5):\n",
        "        avg_maps[d][i] /= float(c)\n",
        "\n",
        "print(\"\\nFinished building per-digit averages (10×5 maps).\")\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 4) Plot the 10×5 grid of average heatmaps (wider figure, title at top)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "os.makedirs(\"figures\", exist_ok=True)\n",
        "\n",
        "fig, axes = plt.subplots(\n",
        "    nrows    = 10,\n",
        "    ncols    = 5,\n",
        "    figsize  = (16, 24),           # Increased width to accommodate the legend\n",
        "    constrained_layout = True\n",
        ")\n",
        "fig.suptitle(\n",
        "    \"Average c-MWP Heatmaps per Digit (rows=0–9) & Feature (cols=X₁–X₅)\",\n",
        "    fontsize = 20,\n",
        "    y        = 0.99                # Push the title to the very top\n",
        ")\n",
        "\n",
        "for d in range(10):\n",
        "    # Compute row-specific vmin/vmax over X₁..X₅ for visibility\n",
        "    row_min = min(np.min(avg_maps[d][i]) for i in range(5))\n",
        "    row_max = max(np.max(avg_maps[d][i]) for i in range(5))\n",
        "\n",
        "    for i in range(5):\n",
        "        ax = axes[d, i]\n",
        "        im = ax.imshow(\n",
        "            avg_maps[d][i],\n",
        "            cmap = \"viridis\",\n",
        "            vmin = row_min,\n",
        "            vmax = row_max\n",
        "        )\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "\n",
        "        if d == 0:\n",
        "            ax.set_title(f\"X{i+1}\", fontsize=14, color=\"white\", backgroundcolor=\"black\")\n",
        "        if i == 0:\n",
        "            ax.set_ylabel(\n",
        "                f\"Digit {d}\",\n",
        "                fontsize = 14,\n",
        "                color    = \"white\",\n",
        "                backgroundcolor = \"black\",\n",
        "                rotation = 0,\n",
        "                labelpad = 50\n",
        "            )\n",
        "\n",
        "# Add a single, tall colorbar on the right\n",
        "cax = fig.add_axes([0.94, 0.05, 0.015, 0.9])  # [left, bottom, width, height]\n",
        "cbar = plt.colorbar(im, cax=cax, orientation=\"vertical\")\n",
        "cbar.set_label(\"Avg c-MWP\", fontsize=12)\n",
        "cbar.ax.tick_params(labelsize=12)\n",
        "\n",
        "out_path = os.path.join(\"figures\", \"average_cmwp_per_digit_wide.png\")\n",
        "plt.savefig(out_path, dpi=180, bbox_inches=\"tight\")\n",
        "plt.close(fig)\n",
        "\n",
        "print(f\"\\nSaved the wide 10×5 grid → {out_path}\\n\")"
      ],
      "id": "823fbdc5",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/briancervantesalvarez/Library/Python/3.11/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}